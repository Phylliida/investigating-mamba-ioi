{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1812c036-831d-4b08-b14a-156ede4cbb7c",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a248986-64cf-433e-8782-9cc0585347f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", font_size=None, show=True, color_continuous_midpoint=0.0, **kwargs):\n",
    "    import plotly.express as px\n",
    "    import transformer_lens.utils as utils\n",
    "    fig = px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=color_continuous_midpoint, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs)\n",
    "    if not font_size is None:\n",
    "        if 'x' in kwargs:\n",
    "            fig.update_layout(\n",
    "              xaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['x'],\n",
    "                ticktext = kwargs['x'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "        if 'y' in kwargs:\n",
    "            fig.update_layout(\n",
    "              yaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['y'],\n",
    "                ticktext = kwargs['y'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "    plot_args = {\n",
    "        'width': 800,\n",
    "        'height': 600,\n",
    "        \"autosize\": False,\n",
    "        'showlegend': True,\n",
    "        'margin': {\"l\":0,\"r\":0,\"t\":100,\"b\":0}\n",
    "    }\n",
    "    \n",
    "    fig.update_layout(**plot_args)\n",
    "    fig.update_layout(legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    ))\n",
    "    if show:\n",
    "        fig.show(renderer)\n",
    "    else:\n",
    "        return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c3e22ed-b7f6-4ed2-b73a-eda2b4e6c365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fba2823af50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# requires\n",
    "# pip install git+https://github.com/Phylliida/MambaLens.git\n",
    "\n",
    "from mamba_lens import HookedMamba # this will take a little while to import\n",
    "import torch\n",
    "model_path = \"state-spaces/mamba-370m\"\n",
    "model = HookedMamba.from_pretrained(model_path, device='cuda')\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1baabd77-6d6d-416c-95a7-8b3d5b314ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using templates\n",
      "Then, [NAME], [NAME] and [NAME] went to the [PLACE]. [NAME] and [NAME] gave a [OBJECT] to\n",
      "Afterwards [NAME], [NAME] and [NAME] went to the [PLACE]. [NAME] and [NAME] gave a [OBJECT] to\n",
      "Friends [NAME], [NAME] and [NAME] went to the [PLACE]. [NAME] and [NAME] gave a [OBJECT] to\n",
      "with name positions (2, 4, 6, 12, 14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from acdc.data.ioi import ioi_data_generator, ABC_TEMPLATES, get_all_single_name_abc_patching_formats\n",
    "from acdc.data.utils import generate_dataset\n",
    "\n",
    "num_patching_pairs = 200\n",
    "seed = 27\n",
    "valid_seed = 28\n",
    "constrain_to_answers = True\n",
    "# this makes our data size 800, first 400 is each (a,b) pair, and then second 400 is each pair swapped to be (b,a)\n",
    "has_symmetric_patching = True\n",
    "\n",
    "from acdc.data.ioi import BABA_TEMPLATES\n",
    "templates = ABC_TEMPLATES\n",
    "patching_formats = list(get_all_single_name_abc_patching_formats())\n",
    "\n",
    "\n",
    "data = generate_dataset(model=model,\n",
    "                  data_generator=ioi_data_generator,\n",
    "                  num_patching_pairs=num_patching_pairs,\n",
    "                  seed=seed,\n",
    "                  valid_seed=valid_seed,\n",
    "                  constrain_to_answers=constrain_to_answers,\n",
    "                  has_symmetric_patching=has_symmetric_patching, \n",
    "                  varying_data_lengths=True,\n",
    "                  templates=templates,\n",
    "                  patching_formats=patching_formats)\n",
    "\n",
    "\n",
    "import acdc.data.ioi\n",
    "from collections import defaultdict\n",
    "name_positions_map = defaultdict(lambda: [])\n",
    "for template in templates:\n",
    "    name = acdc.data.ioi.good_names[0]\n",
    "    template_filled_in = template.replace(\"[NAME]\", name)\n",
    "    template_filled_in = template_filled_in.replace(\"[PLACE]\", acdc.data.ioi.good_nouns['[PLACE]'][0])\n",
    "    template_filled_in = template_filled_in.replace(\"[OBJECT]\", acdc.data.ioi.good_nouns['[OBJECT]'][0])\n",
    "    # get the token positions of the [NAME] in the prompt\n",
    "    name_positions = tuple([(i) for (i,s) in enumerate(model.to_str_tokens(torch.tensor(model.tokenizer.encode(template_filled_in)))) if s == f' {name}'])\n",
    "    name_positions_map[name_positions].append(template)\n",
    "sorted_by_frequency = sorted(list(name_positions_map.items()), key=lambda x: -len(x[1]))\n",
    "most_frequent_name_positions, templates = sorted_by_frequency[0]\n",
    "print(\"using templates\")\n",
    "for template in templates:\n",
    "    print(template)\n",
    "print(f\"with name positions {most_frequent_name_positions}\")\n",
    "\n",
    "data = generate_dataset(model=model,\n",
    "              data_generator=ioi_data_generator,\n",
    "              num_patching_pairs=num_patching_pairs,\n",
    "              seed=seed,\n",
    "              valid_seed=valid_seed,\n",
    "              constrain_to_answers=constrain_to_answers,\n",
    "              has_symmetric_patching=has_symmetric_patching, \n",
    "              varying_data_lengths=True,\n",
    "              templates=templates,\n",
    "              patching_formats=patching_formats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12f2588-c57e-4a9c-a41c-a26de661a26e",
   "metadata": {},
   "source": [
    "# Run Edge Attribution Patching with Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d14fc6b9-7646-449e-ad90-9ddb600e9f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30\n",
      "alpha 0.0 metric 1.0\n",
      "alpha 0.25 metric 0.39601418375968933\n",
      "alpha 0.5 metric 0.08676499873399734\n",
      "alpha 0.75 metric 0.010888022370636463\n",
      "alpha 1.0 metric -5.74837999423039e-09\n",
      "30 60\n",
      "alpha 0.0 metric 1.0\n",
      "alpha 0.25 metric 0.37620702385902405\n",
      "alpha 0.5 metric 0.08132590353488922\n",
      "alpha 0.75 metric 0.01024048775434494\n",
      "alpha 1.0 metric 6.777784733458248e-08\n",
      "60 90\n",
      "alpha 0.0 metric 1.0\n",
      "alpha 0.25 metric 0.39597615599632263\n",
      "alpha 0.5 metric 0.08992055803537369\n",
      "alpha 0.75 metric 0.011775696650147438\n",
      "alpha 1.0 metric 1.2134648841310991e-08\n",
      "90 120\n",
      "alpha 0.0 metric 1.0\n",
      "alpha 0.25 metric 0.3865799903869629\n",
      "alpha 0.5 metric 0.08268946409225464\n",
      "alpha 0.75 metric 0.010380798019468784\n",
      "alpha 1.0 metric 8.326380473988593e-09\n",
      "120 150\n",
      "alpha 0.0 metric 1.0\n",
      "alpha 0.25 metric 0.38332077860832214\n",
      "alpha 0.5 metric 0.08523042500019073\n",
      "alpha 0.75 metric 0.010738296434283257\n",
      "alpha 1.0 metric -9.041420412359003e-08\n",
      "150 180\n",
      "alpha 0.0 metric 1.0\n",
      "alpha 0.25 metric 0.37772563099861145\n",
      "alpha 0.5 metric 0.08146341145038605\n",
      "alpha 0.75 metric 0.010312804020941257\n",
      "alpha 1.0 metric -1.1701491331450597e-07\n",
      "180 210\n",
      "alpha 0.0 metric 1.0\n",
      "alpha 0.25 metric 0.38865962624549866\n",
      "alpha 0.5 metric 0.08416036516427994\n",
      "alpha 0.75 metric 0.010557817295193672\n",
      "alpha 1.0 metric 5.0695131648126335e-08\n",
      "210 240\n",
      "alpha 0.0 metric 1.0\n",
      "alpha 0.25 metric 0.3846832513809204\n",
      "alpha 0.5 metric 0.08451107889413834\n",
      "alpha 0.75 metric 0.010912379249930382\n",
      "alpha 1.0 metric -5.574906580818606e-08\n",
      "240 270\n",
      "alpha 0.0 metric 1.0\n",
      "alpha 0.25 metric 0.36897820234298706\n",
      "alpha 0.5 metric 0.08038737624883652\n",
      "alpha 0.75 metric 0.00999701488763094\n",
      "alpha 1.0 metric -1.073063700118837e-07\n",
      "270 300\n",
      "alpha 0.0 metric 1.0\n",
      "alpha 0.25 metric 0.36401644349098206\n",
      "alpha 0.5 metric 0.08026204258203506\n",
      "alpha 0.75 metric 0.010135699063539505\n",
      "alpha 1.0 metric -1.2533594995201014e-10\n",
      "300 330\n",
      "alpha 0.0 metric 1.0\n",
      "alpha 0.25 metric 0.37994205951690674\n",
      "alpha 0.5 metric 0.08327409625053406\n",
      "alpha 0.75 metric 0.010647255927324295\n",
      "alpha 1.0 metric -1.1392976517754505e-07\n",
      "330 360\n",
      "alpha 0.0 metric 1.0\n",
      "alpha 0.25 metric 0.39947277307510376\n",
      "alpha 0.5 metric 0.09096205979585648\n",
      "alpha 0.75 metric 0.011631865985691547\n",
      "alpha 1.0 metric -1.949799788292239e-08\n",
      "360 390\n",
      "alpha 0.0 metric 1.0\n",
      "alpha 0.25 metric 0.37203606963157654\n",
      "alpha 0.5 metric 0.08260570466518402\n",
      "alpha 0.75 metric 0.0107108810916543\n",
      "alpha 1.0 metric -1.2769334034601343e-07\n",
      "390 400\n",
      "alpha 0.0 metric 1.0\n",
      "alpha 0.25 metric 0.36545225977897644\n",
      "alpha 0.5 metric 0.07713738083839417\n",
      "alpha 0.75 metric 0.009796429425477982\n",
      "alpha 1.0 metric -1.2324353804160637e-07\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "from functools import partial\n",
    "from jaxtyping import Float\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from mamba_lens.input_dependent_hooks import clean_hooks\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_kwargs = {\n",
    "    'fast_ssm': False,\n",
    "    'fast_conv': True\n",
    "}\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "# removes all hooks including \"leftover\" ones that might stick around due to interrupting the model at certain times\n",
    "clean_hooks(model)\n",
    "\n",
    "def normalized_logit_diff_metric(patched_logits, unpatched_logits, corrupted_logits, patched_correct, corrupted_correct, also_return_acc=False):\n",
    "    B,V = patched_logits.size()\n",
    "\n",
    "    #print(data.unpatched.logits.size(), data.patched.logits.size(), data.corrupted.logits.size())\n",
    "    A_logits_unpatched = unpatched_logits[torch.arange(B), patched_correct]\n",
    "    A_logits_patched = patched_logits[torch.arange(B), patched_correct]\n",
    "    A_logits_corrupted = corrupted_logits[torch.arange(B), patched_correct]\n",
    "\n",
    "    B_logits_unpatched = unpatched_logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_patched = patched_logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_corrupted = corrupted_logits[torch.arange(B), corrupted_correct]\n",
    "\n",
    "    # A and B are two potential outputs\n",
    "    # if A patched > B patched, we are correct\n",
    "    # else we are incorrect\n",
    "\n",
    "    # thus we could just return A_logits_patched - B_logits_patched\n",
    "\n",
    "    # however it is useful to \"normalize\" these values\n",
    "\n",
    "    # in the worst case, our patching causes us to act like corrupted, and our diff will be\n",
    "    # A_logits_corrupted - B_logits_corrupted\n",
    "    # this will result in a small, negative value\n",
    "    \n",
    "    # in the best case, our patching will do nothing (cause us to act like unpatched), and our diff will be\n",
    "    # A_logits_unpatched - B_logits_unpatched\n",
    "    # this will result in a large, positive value\n",
    "\n",
    "    # thus we can treat those as the \"min\" and \"max\" and normalize accordingly\n",
    "    \n",
    "    min_diff = A_logits_corrupted - B_logits_corrupted\n",
    "    max_diff = A_logits_unpatched - B_logits_unpatched\n",
    "\n",
    "    # the abs ensures that if it's wrong we don't try and make it more wrong\n",
    "    possible_range = torch.abs(max_diff-min_diff)\n",
    "    possible_range[possible_range == 0] = 1.0 # prevent divide by zero\n",
    "    \n",
    "    diff = A_logits_patched - B_logits_patched\n",
    "    \n",
    "    normalized_diff = (diff-min_diff)/possible_range\n",
    "\n",
    "    # as described, 1.0 corresponds to acting like unpatched,\n",
    "    # and 0.0 corresponds to acting like corrupted\n",
    "    res = torch.mean(normalized_diff)\n",
    "    \n",
    "    if also_return_acc:\n",
    "        num_correct = A_logits_patched > B_logits_patched\n",
    "        acc = torch.sum(num_correct)/B\n",
    "        return res, acc\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "\n",
    "# there's a subtle bug if you aren't careful:\n",
    "# consider what happens when we do edge attribution patching and patch every edge\n",
    "# what we want to happen is that it's identical to corrupted\n",
    "# however this is not what happens!\n",
    "# Start with layer 0:\n",
    "# layer 0 will be patched\n",
    "#    we subtract uncorrupted embed and add corrupted embed\n",
    "#    in other words, the embed input to layer 0 will be from the corrupted run\n",
    "# this results in layer 0 having an output of corrupted, as desired\n",
    "# now consider layer 1\n",
    "#    subtract uncorrupted embed and add corrupted embed\n",
    "#      this is fine and results in embed input to layer 1 of corrupted\n",
    "#    subtract uncorrupted layer 0 and add corrupted layer 0\n",
    "#      layer 0 is already corrupted, so this has the effect of the output of layer 0 being\n",
    "#          2*corrupted layer 0 - uncorrupted layer 0\n",
    "#          this is not the same as corrupted!\n",
    "\n",
    "# two ways to fix this:\n",
    "# 1. fetch stored layer_input from uncorrupted run, and use that instead of the layer_input given in fwd_diff\n",
    "#   this works, but then the gradients won't propogate properly (maybe? todo: test)\n",
    "# 2. mark which edges are patched and don't \"double patch\" them\n",
    "#   if we are patching all edges, this means that we simply apply only the embed diff to all layers,\n",
    "#   as that'll result in all layers being patched\n",
    "# 3. just subtract the outputs in the same forward pass, instead of a cached \"unpatched\" run\n",
    "#  we do 3\n",
    "\n",
    "global alpha\n",
    "alpha = 1\n",
    "\n",
    "def cache_output_hook( # hook_layer_output\n",
    "    layer_output : Float[torch.Tensor, \"B L D\"],\n",
    "    hook : HookPoint,\n",
    "    layer : int,\n",
    "    cached_outputs : Float[torch.Tensor, \"NLayers+1 B L D\"]):\n",
    "    global alpha\n",
    "    cached_outputs[layer+1] = layer_output\n",
    "    return layer_output\n",
    "\n",
    "def fwd_diff_hook( # hook_layer_input\n",
    "    layer_input : Float[torch.Tensor, \"B L D\"],\n",
    "    hook : HookPoint,\n",
    "    layer : int,\n",
    "    cached_outputs : Float[torch.Tensor, \"NLayers+1 B L D\"],\n",
    "    corrupted_outputs : Float[torch.Tensor, \"NLayers+1 B L D\"]):\n",
    "    global alpha\n",
    "    return layer_input + (-cached_outputs[:layer+1]+corrupted_outputs[:layer+1]).sum(dim=0)*alpha\n",
    "\n",
    "def bwd_diff_hook( # hook_layer_input\n",
    "    grad : Float[torch.Tensor, \"B L D\"],\n",
    "    hook: HookPoint,\n",
    "    layer : int,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    cached_outputs : Float[torch.Tensor, \"NLayers+1 B L D\"],\n",
    "    corrupted_outputs : Float[torch.Tensor, \"NLayers+1 B L D\"]):\n",
    "    #print(f\"running bwd with alpha {alpha} and layer {layer}\") \n",
    "    \n",
    "    # [N_upstream, B, L, D]\n",
    "    upstream_diffs = (-cached_outputs[:layer+1]+corrupted_outputs[:layer+1])\n",
    "    # grad is [B,L,D]\n",
    "\n",
    "    # to do a taylor approximation of metric with respect to diff_0, we\n",
    "    # multiply diffs and grad, then\n",
    "    # sum over the L and D dimensions (this is doing a dot product of vectors of size L*D)\n",
    "    # now we have attr of size [N_upstream, B, L]\n",
    "    attr = (grad*upstream_diffs).sum(dim=-1)\n",
    "    # [B, N_upstream, L]           [N_upstream, B, L]\n",
    "    attr         =   torch.transpose(attr, 0, 1)\n",
    "    # [B, N_upstream, L]\n",
    "    attr = attr.clone().detach()\n",
    "    if POSITIONS:\n",
    "        attributions[batch_start:batch_end,:layer+1,layer+1] += attr\n",
    "    else:\n",
    "        #[B, N_upstream]\n",
    "        attr = attr.sum(dim=-1)\n",
    "        attributions[batch_start:batch_end,:layer+1,layer+1] += attr\n",
    "\n",
    "B,L = data.data.size()\n",
    "# our data is pairs of unpatched, corrupted\n",
    "n_patching_pairs = B//2\n",
    "\n",
    "# attributions[b,i+1,j+1] is the (i->j) edge attribution for patching pair b\n",
    "# attributions[b,0,j] is the (embed->j) edge attribution for patching pair b\n",
    "# attributions[b,i,-1] is the (i->output) edge attribution for patching pair b\n",
    "POSITIONS = False\n",
    "if POSITIONS:\n",
    "    attributions = torch.zeros([n_patching_pairs, model.cfg.n_layers+2, model.cfg.n_layers+2, L], device=model.cfg.device)\n",
    "else:\n",
    "    attributions = torch.zeros([n_patching_pairs, model.cfg.n_layers+2, model.cfg.n_layers+2], device=model.cfg.device)\n",
    "\n",
    "input_names = [f'blocks.{i}.hook_layer_input' for i in range(model.cfg.n_layers)]\n",
    "output_names = [f'blocks.{i}.hook_out_proj' for i in range(model.cfg.n_layers)]\n",
    "\n",
    "\n",
    "for batch_start in range(0, n_patching_pairs, BATCH_SIZE):\n",
    "    batch_end = min(batch_start + BATCH_SIZE, n_patching_pairs)\n",
    "    print(batch_start, batch_end)\n",
    "    # we don't need grad for these forward passes\n",
    "    torch.set_grad_enabled(False)\n",
    "    embed_name = 'hook_embed'\n",
    "\n",
    "    clean_hooks(model)\n",
    "    # forward passes to get unpatched and corrupted\n",
    "    unpatched_logits, unpatched_layer_outputs = model.run_with_cache(data.data[::2][batch_start:batch_end], names_filter=[embed_name] + output_names, **model_kwargs)\n",
    "    corrupted_logits, corrupted_layer_outputs = model.run_with_cache(data.data[1::2][batch_start:batch_end], names_filter=[embed_name] + output_names, **model_kwargs)\n",
    "    \n",
    "    batch_size,L,D = unpatched_layer_outputs[output_names[0]].size()\n",
    "    \n",
    "    # get only the last token position (logit for next predicted token)\n",
    "    # this is needed to support data of varying lengths\n",
    "    unpatched_logits = unpatched_logits[torch.arange(batch_size), data.last_token_position[::2][batch_start:batch_end]]\n",
    "    corrupted_logits = corrupted_logits[torch.arange(batch_size), data.last_token_position[1::2][batch_start:batch_end]]\n",
    "    \n",
    "    clean_hooks(model)\n",
    "\n",
    "    # backward pass to compute grad of diff\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    \n",
    "    corrupted_outputs = torch.zeros([model.cfg.n_layers+1,batch_size,L,D], device=model.cfg.device)\n",
    "    corrupted_outputs.requires_grad = False\n",
    "    # first one is for embed\n",
    "    corrupted_outputs[0] = corrupted_layer_outputs[embed_name]\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        output_name = output_names[layer]\n",
    "        corrupted_outputs[layer+1] = corrupted_layer_outputs[output_name]\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "        param.grad = None # reset grads\n",
    "    \n",
    "    last_layer = model.cfg.n_layers-1\n",
    "    # forward pass to do partial patches\n",
    "    cached_outputs = torch.zeros([model.cfg.n_layers+1,batch_size,L,D], device=model.cfg.device)\n",
    "    cached_outputs.requires_grad = False\n",
    "    cache_output_hooks = [(embed_name,\n",
    "                           partial(cache_output_hook,\n",
    "                                   layer=-1,\n",
    "                                   cached_outputs=cached_outputs))]\n",
    "    \n",
    "    cache_output_hooks += [(output_names[layer],\n",
    "                            partial(cache_output_hook,\n",
    "                                    layer=layer,\n",
    "                                    cached_outputs=cached_outputs)) for layer in range(model.cfg.n_layers)]\n",
    "    \n",
    "    fwd_hooks = cache_output_hooks\n",
    "    fwd_hooks += [(input_names[layer],\n",
    "                  partial(\n",
    "                      fwd_diff_hook,\n",
    "                      layer=layer,\n",
    "                      cached_outputs=cached_outputs,\n",
    "                      corrupted_outputs=corrupted_outputs,\n",
    "                  )) for layer in range(model.cfg.n_layers)]\n",
    "    bwd_hooks = [(input_names[layer],\n",
    "                  partial(bwd_diff_hook,\n",
    "                          layer=layer,\n",
    "                          cached_outputs=cached_outputs,\n",
    "                          corrupted_outputs=corrupted_outputs,\n",
    "                          batch_start=batch_start,\n",
    "                          batch_end=batch_end)) for layer in range(model.cfg.n_layers)]\n",
    "    # extra hook for the very last layer\n",
    "    fwd_hooks.append((f'blocks.{last_layer}.hook_resid_post',\n",
    "                      partial(fwd_diff_hook,\n",
    "                              layer=last_layer+1,\n",
    "                              cached_outputs=cached_outputs,\n",
    "                              corrupted_outputs=corrupted_outputs,\n",
    "                             )))\n",
    "    bwd_hooks.append((f'blocks.{last_layer}.hook_resid_post',\n",
    "                      partial(bwd_diff_hook,\n",
    "                              layer=last_layer+1,\n",
    "                              cached_outputs=cached_outputs,\n",
    "                              corrupted_outputs=corrupted_outputs,\n",
    "                              batch_start=batch_start,\n",
    "                              batch_end=batch_end,\n",
    "                             )))\n",
    "\n",
    "    for fwd in fwd_hooks:\n",
    "        model.add_hook(*fwd, \"fwd\")\n",
    "\n",
    "    for bwd in bwd_hooks:\n",
    "        model.add_hook(*bwd, \"bwd\")\n",
    "    \n",
    "    # with integrated gradients\n",
    "    # simply sums over doing \"partial patches\" like 0.2 patch and 0.8 unpatched \n",
    "    # ITERS = 1 is just edge attribution patching (without integraded gradients)\n",
    "    ITERS = 5\n",
    "    for i in range(ITERS):\n",
    "        global alpha\n",
    "        # alpha ranges from 0 to 1\n",
    "        if ITERS > 1:\n",
    "            alpha = i/float(ITERS-1)\n",
    "        elif ITERS == 1: # no integrated gradients, set alpha to 1\n",
    "            alpha = 1.0\n",
    "\n",
    "        # it tries to propogate gradients to these, detach them\n",
    "        cached_outputs[:] = 0\n",
    "        cached_outputs.grad = None\n",
    "        cached_outputs.detach_()\n",
    "        corrupted_outputs.grad = None\n",
    "        corrupted_outputs.detach_()\n",
    "        model.zero_grad()\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        logits = model(data.data[::2][batch_start:batch_end], **model_kwargs)\n",
    "        logits = logits[torch.arange(batch_size), data.last_token_position[::2][batch_start:batch_end]]\n",
    "        metric = normalized_logit_diff_metric(\n",
    "            patched_logits=logits,\n",
    "            unpatched_logits=unpatched_logits,\n",
    "            corrupted_logits=corrupted_logits,\n",
    "            patched_correct=data.correct[::2][batch_start:batch_end][:,0],\n",
    "            corrupted_correct=data.correct[1::2][batch_start:batch_end][:,0]\n",
    "        )\n",
    "        print(f\"alpha {alpha} metric {metric}\")\n",
    "        # run backward pass, which adds to attributions\n",
    "        metric.backward()\n",
    "\n",
    "# todo: maybe the diffs should have alpha in the backward pass? No, that would mean alpha of 0 gives all zero attrs\n",
    "\n",
    "# average over all the samples\n",
    "attributions[:] = attributions[:]/ITERS\n",
    "\n",
    "\n",
    "# don't need grad for rest of this\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "clean_hooks(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab16482e-a3a9-4985-99ad-e7fff25d0b3a",
   "metadata": {},
   "source": [
    "# Binary search to find circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935d17f1-3e73-4168-a318-bda41ee8a07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "got edges\n",
      "testing pos 612\n",
      "testing pos 612 got metric 0.9686607122421265 and acc 0.938095211982727\n",
      "testing pos 306\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "eval_kwargs = {\n",
    "    \"fast_ssm\": True,\n",
    "    \"fast_conv\": True\n",
    "}\n",
    "\n",
    "if POSITIONS:\n",
    "    positions = list(range(L))\n",
    "else:\n",
    "    positions = [None]\n",
    "\n",
    "def test_graph(keeping_edges, valid=False):\n",
    "    edges_patching = []\n",
    "    keeping_edges_set = set(keeping_edges)\n",
    "    \n",
    "    for pos in positions:\n",
    "        for i in range(model.cfg.n_layers):\n",
    "            edge = ('embed', i, pos)\n",
    "            if not edge in keeping_edges_set:\n",
    "                edges_patching.append(edge)\n",
    "            for j in range(i+1, model.cfg.n_layers):\n",
    "                edge = (i, j, pos)\n",
    "                if not edge in keeping_edges_set:\n",
    "                    edges_patching.append(edge)\n",
    "            edge = (i, 'output', pos)\n",
    "            if not edge in keeping_edges_set:\n",
    "                edges_patching.append(edge)\n",
    "        edge = ('embed', 'output', pos)\n",
    "        if not edge in keeping_edges_set:\n",
    "            edges_patching.append(edge)\n",
    "    \n",
    "    storage = {}\n",
    "    clean_hooks(model)\n",
    "\n",
    "    metrics = []\n",
    "    accs = []\n",
    "    for batch_start in range(0, n_patching_pairs, BATCH_SIZE):\n",
    "        batch_end = min(batch_start + BATCH_SIZE, n_patching_pairs)\n",
    "        batch_size = batch_end-batch_start\n",
    "        clean_hooks(model)\n",
    "        if valid:\n",
    "            unpatched_logits, unpatched_layer_outputs = model.run_with_cache(data.valid_data[::2][batch_start:batch_end], names_filter=['hook_embed'] + output_names, **eval_kwargs)\n",
    "            corrupted_logits, corrupted_layer_outputs = model.run_with_cache(data.valid_data[1::2][batch_start:batch_end], names_filter=['hook_embed'] + output_names, **eval_kwargs)\n",
    "            # get only the last token position (logit for next predicted token)\n",
    "            unpatched_logits = unpatched_logits[torch.arange(batch_size), data.valid_last_token_position[::2][batch_start:batch_end]]\n",
    "            corrupted_logits = corrupted_logits[torch.arange(batch_size), data.valid_last_token_position[1::2][batch_start:batch_end]]\n",
    "        else:\n",
    "            unpatched_logits, unpatched_layer_outputs = model.run_with_cache(data.data[::2][batch_start:batch_end], names_filter=['hook_embed'] + output_names, **eval_kwargs)\n",
    "            corrupted_logits, corrupted_layer_outputs = model.run_with_cache(data.data[1::2][batch_start:batch_end], names_filter=['hook_embed'] + output_names, **eval_kwargs)\n",
    "            # get only the last token position (logit for next predicted token)\n",
    "            unpatched_logits = unpatched_logits[torch.arange(batch_size), data.last_token_position[::2][batch_start:batch_end]]\n",
    "            corrupted_logits = corrupted_logits[torch.arange(batch_size), data.last_token_position[1::2][batch_start:batch_end]]\n",
    "        global cache\n",
    "        cache = {}\n",
    "        def caching_hook(\n",
    "            x,\n",
    "            hook):\n",
    "            global cache\n",
    "            cache[hook.name] = x\n",
    "        \n",
    "        # for every edge (i->j) that we patch, we should:\n",
    "        # to compute the input for layer j:\n",
    "        # subtract current forward pass output of layer i\n",
    "        # and add cached corrupted output of layer i\n",
    "        # that has the effect of removing whatever that layer might have outputted\n",
    "        # and instead, the outputs from that \n",
    "        def patching_hook(\n",
    "            x,\n",
    "            hook,\n",
    "            edge_input_hook,\n",
    "            position):\n",
    "            global cache\n",
    "            #print(hook.name, edge_input_hook)\n",
    "            if position is None:\n",
    "                x = x - cache[edge_input_hook] + corrupted_layer_outputs[edge_input_hook] \n",
    "            else:\n",
    "                x[:,position] = x[:,position] - cache[edge_input_hook][:,position] + corrupted_layer_outputs[edge_input_hook][:,position]\n",
    "            return x\n",
    "\n",
    "        # need to handle the embed -> i, i->j, and j->output\n",
    "        def get_edge_input_hook(input_layer):\n",
    "            if input_layer == 'embed':\n",
    "                return f'hook_embed'\n",
    "            else:\n",
    "                return f'blocks.{input_layer}.hook_out_proj'\n",
    "\n",
    "        def get_edge_output_hook(output_layer):\n",
    "            if output_layer == 'output':\n",
    "                return f'blocks.{model.cfg.n_layers-1}.hook_resid_post'\n",
    "            else:\n",
    "                return f'blocks.{output_layer}.hook_layer_input'\n",
    "        hooks = []\n",
    "        cache = {}\n",
    "        for input_layer, output_layer, position in edges_patching:            \n",
    "            hooks.append((get_edge_output_hook(output_layer), partial(patching_hook, edge_input_hook=get_edge_input_hook(input_layer), position=position)))\n",
    "\n",
    "        hooks.append((get_edge_input_hook('embed'), partial(caching_hook)))\n",
    "        for layer in range(model.cfg.n_layers):\n",
    "            hooks.append((get_edge_input_hook(layer), partial(caching_hook)))\n",
    "        \n",
    "        clean_hooks(model)\n",
    "        if valid:\n",
    "            patched_logits = model.run_with_hooks(data.valid_data[::2][batch_start:batch_end], fwd_hooks=hooks, **model_kwargs)\n",
    "            # get only the last token position (logit for next predicted token)\n",
    "            patched_logits = patched_logits[torch.arange(batch_size), data.valid_last_token_position[::2][batch_start:batch_end]]\n",
    "            \n",
    "            metric = normalized_logit_diff_metric(\n",
    "                patched_logits=patched_logits,\n",
    "                unpatched_logits=unpatched_logits,\n",
    "                corrupted_logits=corrupted_logits,\n",
    "                patched_correct=data.valid_correct[::2][batch_start:batch_end][:,0],\n",
    "                corrupted_correct=data.valid_correct[1::2][batch_start:batch_end][:,0],\n",
    "                also_return_acc=False\n",
    "            )\n",
    "            from acdc import index_into, get_pad_token\n",
    "            \n",
    "            pad = get_pad_token(tokenizer=model.tokenizer)\n",
    "            patched_logits[:,pad] = -torch.inf # manually set pad pr to -inf logit because sometimes we need to pad num correct or num incorrect\n",
    "\n",
    "            correct_logits = index_into(patched_logits, data.valid_correct[::2][batch_start:batch_end])\n",
    "            incorrect_logits = index_into(patched_logits, data.valid_incorrect[::2][batch_start:batch_end])\n",
    "            _, n_correct = data.valid_correct.size()\n",
    "            _, n_incorrect = data.valid_incorrect.size()\n",
    "            # [n_data, n_correct + n_incorrect]\n",
    "            combined_logits = torch.concatenate([correct_logits, incorrect_logits], dim=1)\n",
    "            combined_prs = torch.softmax(combined_logits, dim=1)\n",
    "            biggest = torch.argsort(-combined_prs, dim=1)\n",
    "            # if biggest pr is in the correct, we are correct, otherwise, we are not\n",
    "            top_is_correct = biggest[:,0] < n_correct\n",
    "            correct_prs, incorrect_prs = combined_prs.split([n_correct, n_incorrect], dim=1)\n",
    "            acc = torch.sum(top_is_correct) / float(batch_end-batch_start)\n",
    "        else:\n",
    "            patched_logits = model.run_with_hooks(data.data[::2][batch_start:batch_end], fwd_hooks=hooks, **model_kwargs)\n",
    "            # get only the last token position (logit for next predicted token)\n",
    "            patched_logits = patched_logits[torch.arange(batch_size), data.last_token_position[::2][batch_start:batch_end]]\n",
    "                \n",
    "            metric = normalized_logit_diff_metric(\n",
    "                patched_logits=patched_logits,\n",
    "                unpatched_logits=unpatched_logits,\n",
    "                corrupted_logits=corrupted_logits,\n",
    "                patched_correct=data.correct[::2][batch_start:batch_end][:,0],\n",
    "                corrupted_correct=data.correct[1::2][batch_start:batch_end][:,0],\n",
    "                also_return_acc=False\n",
    "            )\n",
    "            from acdc import index_into, get_pad_token\n",
    "            \n",
    "            pad = get_pad_token(tokenizer=model.tokenizer)\n",
    "            patched_logits[:,pad] = -torch.inf # manually set pad pr to -inf logit because sometimes we need to pad num correct or num incorrect\n",
    "\n",
    "            correct_logits = index_into(patched_logits, data.correct[::2][batch_start:batch_end])\n",
    "            incorrect_logits = index_into(patched_logits, data.incorrect[::2][batch_start:batch_end])\n",
    "            _, n_correct = data.correct.size()\n",
    "            _, n_incorrect = data.incorrect.size()\n",
    "            # [n_data, n_correct + n_incorrect]\n",
    "            combined_logits = torch.concatenate([correct_logits, incorrect_logits], dim=1)\n",
    "            combined_prs = torch.softmax(combined_logits, dim=1)\n",
    "            biggest = torch.argsort(-combined_prs, dim=1)\n",
    "            # if biggest pr is in the correct, we are correct, otherwise, we are not\n",
    "            top_is_correct = biggest[:,0] < n_correct\n",
    "            correct_prs, incorrect_prs = combined_prs.split([n_correct, n_incorrect], dim=1)\n",
    "            acc = torch.sum(top_is_correct) / float(batch_end-batch_start)\n",
    "    \n",
    "        metrics.append(metric.item())\n",
    "        accs.append(acc.item())\n",
    "\n",
    "    metric, acc = torch.mean(torch.tensor(metrics)), torch.mean(torch.tensor(accs))\n",
    "    return metric, acc\n",
    "\n",
    "# mean over all batches\n",
    "attrs = attributions.mean(dim=0)\n",
    "\n",
    "_, L = data.data.size()\n",
    "edges = []\n",
    "for pos in positions:\n",
    "    print(pos)\n",
    "    # embed -> i\n",
    "    for i in range(model.cfg.n_layers):\n",
    "        edges.append((attrs[0,i+1,pos].flatten()[0], 'embed', i, pos))\n",
    "    # i -> j\n",
    "    for i in range(model.cfg.n_layers):\n",
    "        for j in range(i+1, model.cfg.n_layers):\n",
    "            edges.append((attrs[i+1,j+1,pos].flatten()[0], i, j, pos))\n",
    "    # j -> output\n",
    "    for j in range(model.cfg.n_layers):\n",
    "        edges.append((attrs[j+1,model.cfg.n_layers+1,pos].flatten()[0], j, 'output', pos))\n",
    "    # embed -> output\n",
    "    edges.append((attrs[0,model.cfg.n_layers+1,pos].flatten()[0], 'embed', 'output', pos))\n",
    "print(\"got edges\")\n",
    "\n",
    "# can also do this instead to include negative contributions but I find the graph is larger, ymmv\n",
    "#edges.sort(key=lambda x: -abs(x[0]))\n",
    "edges.sort(key=lambda x: x[0])\n",
    "\n",
    "edges_with_improvements = []\n",
    "    \n",
    "prev_metric = 0\n",
    "\n",
    "# binary search for minimum number of edges that passes desired threshold\n",
    "pos = len(edges)//2\n",
    "jump_size = len(edges)//4\n",
    "\n",
    "import math\n",
    "def test_pos(pos):\n",
    "    print(f\"testing pos {pos}\")\n",
    "    edges_to_keep = [(in_edge,out_edge,posf) for (score, in_edge, out_edge, posf) in edges[:pos]]\n",
    "    metric, acc = test_graph(edges_to_keep)\n",
    "    print(f\"testing pos {pos} got metric {metric} and acc {acc}\")\n",
    "    return acc.item()\n",
    "\n",
    "# from https://en.wikipedia.org/wiki/Binary_search_algorithm\n",
    "def binary_search(n, T):\n",
    "    L = 0\n",
    "    R = n - 1\n",
    "    while L != R:\n",
    "        m = math.ceil((L + R) / 2)\n",
    "        if test_pos(m) > T:\n",
    "            R = m - 1\n",
    "        else:\n",
    "            L = m\n",
    "    # go one further because this gives us below thresh\n",
    "    return min(n-1, L+1)\n",
    "\n",
    "ACC_THRESH = 0.85\n",
    "\n",
    "cutoff = binary_search(len(edges), T=ACC_THRESH)\n",
    "\n",
    "edges_to_keep = [(in_edge,out_edge,pos) for (score, in_edge, out_edge,pos) in edges[:cutoff]]\n",
    "scores = [score for (score, in_edge, out_edge,pos) in edges[:cutoff]]\n",
    "print(f\"keeping top {cutoff} edges {edges_to_keep}\")\n",
    "metric, acc = test_graph(edges_to_keep)\n",
    "print(f\"got metric {metric} and acc {acc}\")    \n",
    "\n",
    "metric, acc = test_graph(edges_to_keep, valid=True)\n",
    "print(f\"got valid metric {metric} and acc {acc}\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d44d5a-087d-4663-9314-8abf80ee3389",
   "metadata": {},
   "source": [
    "# Display EAP results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084c1b5-26e3-4c9d-b798-c19537303627",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = list(map(str, range(model.cfg.n_layers)))\n",
    "layer_names = ['embed'] + layer_names + ['output']\n",
    "imshow(attributions.mean(dim=0), x=layer_names, y=layer_names, color_continuous_midpoint=0, font_size=10, title='IOI normalized logit diff edge attribution patching')\n",
    "\n",
    "# feel free to change this\n",
    "CAP = -0.0001\n",
    "\n",
    "def edge_to_i(edge):\n",
    "    if edge == 'embed': return 0\n",
    "    elif edge == 'output': return model.cfg.n_layers+1\n",
    "    else: return edge+1\n",
    "\n",
    "_, L = data.data.size()\n",
    "if not POSITIONS:\n",
    "    L = 1\n",
    "adjacency_matrix = torch.zeros([model.cfg.n_layers+2,model.cfg.n_layers+2,L])\n",
    "for score, (i,o,pos) in pruned_edges:\n",
    "    adjacency_matrix[edge_to_i(i),edge_to_i(o),pos] = max(score, CAP)\n",
    "layer_names = ['embed'] + list(map(str, range(model.cfg.n_layers))) + ['output']\n",
    "for l in range(L):\n",
    "    pos_str = f' for pos {l}'\n",
    "    if not POSITIONS:\n",
    "        pos_str = \"\"\n",
    "    imshow(adjacency_matrix[:,:,l], x=layer_names, y=layer_names, color_continuous_midpoint=0, font_size=10, title=f'Adjency Matrix ({len(edges_to_keep)} edges) of minimal graph found by EAP with acc {ACC_THRESH} capped to -0.0001')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db660aca-b3b6-4c78-b083-4ede2ca9f016",
   "metadata": {},
   "source": [
    "# Prune nodes that don't have a input -> node -> output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402d328-3520-47d9-bc33-1aebb1eaa98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx\n",
    "def get_nx_graph(edges) -> nx.DiGraph:\n",
    "    '''\n",
    "    Converts the edges into a networkx graph\n",
    "    only edges that have checked == True and patching == False are included\n",
    "    if include_unchecked=True, any edge that has checked == False is also included\n",
    "    '''\n",
    "    G = nx.DiGraph()\n",
    "    for score, (edge_input, edge_output, pos) in edges:\n",
    "        G.add_edge(str(edge_input), str(edge_output), label=str(pos))\n",
    "    return G\n",
    "\n",
    "def prune_edges(edges, input_node, output_node):\n",
    "    import networkx as nx\n",
    "    G = get_nx_graph(edges=edges)\n",
    "    connected_edges = []\n",
    "    for score, (edge_input, edge_output,pos) in edges:\n",
    "        connected_to_input = False\n",
    "        try:\n",
    "            to_input = nx.shortest_path(G, source=str(input_node), target=str(edge_input))\n",
    "            connected_to_input = True\n",
    "        except nx.NetworkXNoPath:\n",
    "            pass\n",
    "        except nx.NodeNotFound:\n",
    "            raise ValueError(f\"Graph does not have input node {input_node}\")\n",
    "        \n",
    "        connected_to_output = False\n",
    "        try:\n",
    "            to_output = nx.shortest_path(G, source=str(edge_output), target=str(output_node))\n",
    "            connected_to_output = True\n",
    "        except nx.NetworkXNoPath:\n",
    "            pass\n",
    "        except nx.NodeNotFound:\n",
    "            raise ValueError(f\"Graph does not have output node {output_node}\")\n",
    "        if connected_to_input and connected_to_output:\n",
    "            connected_edges.append((score, (edge_input, edge_output,pos)))\n",
    "    return connected_edges\n",
    "\n",
    "\n",
    "import graphviz\n",
    "from IPython.display import Image, display, clear_output, FileLink\n",
    "\n",
    "def top_4_digits(f):\n",
    "    return \"{:.4f}\".format(f)\n",
    "\n",
    "pruned_edges = prune_edges(list(zip(scores, edges_to_keep)), input_node='embed', output_node='output')\n",
    "\n",
    "pruned_dot = graphviz.Digraph('graph')\n",
    "for score, (i,o,pos) in pruned_edges:\n",
    "    if pos is None:\n",
    "        pruned_dot.edge(str(i),str(o), label=top_4_digits(score))\n",
    "    else:\n",
    "        pruned_dot.edge(str(i),str(o), label=f\"{pos}[{top_4_digits(score)}]\")\n",
    "output_name = f'pruned dot {ACC_THRESH}'\n",
    "pruned_dot.render(output_name, format=\"png\") # it automatically appends png\n",
    "display(Image(filename=output_name + \".png\"))\n",
    "display(FileLink(output_name + \".png\"))\n",
    "\n",
    "dot = graphviz.Digraph('graph')\n",
    "for score, (i,o,pos) in zip(scores, edges_to_keep):\n",
    "    if pos is None:\n",
    "        dot.edge(str(i),str(o),label=top_4_digits(score))\n",
    "    else:\n",
    "        dot.edge(str(i),str(o),label=f\"{pos}[{top_4_digits(score)}]\")\n",
    "metric, acc = test_graph([x[1] for x in pruned_edges])\n",
    "print(f\"pruned graph gets metric {metric} and acc {acc}\")\n",
    "metric, acc = test_graph(edges_to_keep)\n",
    "print(f\"unpruned graph gets metric {metric} and acc {acc}\")\n",
    "output_name = f'dot {ACC_THRESH}'\n",
    "dot.render(output_name, format=\"png\") # it automatically appends png\n",
    "display(Image(filename=output_name + \".png\"))\n",
    "display(FileLink(output_name + \".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ab148-7349-4943-a2d5-da97d32e10f5",
   "metadata": {},
   "source": [
    "# ACDC, starting from the EAP edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746dce2-17f3-4c5a-85e6-25da27484b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "import os\n",
    "import signal\n",
    "import acdc\n",
    "from tqdm import tqdm\n",
    "from typing import Any  \n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from acdc import (\n",
    "    Edge,\n",
    "    ACDCConfig,\n",
    "    LOG_LEVEL_INFO,\n",
    "    LOG_LEVEL_DEBUG,\n",
    "    run_acdc,\n",
    "    ACDCEvalData,\n",
    "    load_checkpoint,\n",
    "    get_most_recent_checkpoint\n",
    ")\n",
    "\n",
    "\n",
    "from acdc import get_pad_token\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "global storage\n",
    "def storage_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    **kwargs,\n",
    "):\n",
    "    global storage\n",
    "    #if hook.name == 'hook_embed':\n",
    "    #    for k in list(storage.keys()):\n",
    "    #        del storage[k]\n",
    "    storage[hook.name] = x\n",
    "    return x\n",
    "\n",
    "def resid_patching_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    position: int = None,\n",
    "):\n",
    "    global storage\n",
    "    x_uncorrupted = storage[input_hook_name][batch_start:batch_end:2]\n",
    "    x_corrupted = storage[input_hook_name][batch_start+1:batch_end:2]\n",
    "    if position is None: # if position not specified, apply to all positions\n",
    "        x[batch_start:batch_end:2] = x[batch_start:batch_end:2] - x_uncorrupted + x_corrupted\n",
    "    else:\n",
    "        x[batch_start:batch_end:2,position] = x[batch_start:batch_end:2,position] - x_uncorrupted[:,position] + x_corrupted[:,position]\n",
    "    return x\n",
    "\n",
    "def overwrite_patching_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    position: int = None,\n",
    "):\n",
    "    x_corrupted = x[batch_start+1:batch_end:2]\n",
    "    if position is None: # if position not specified, apply to all positions\n",
    "        x[batch_start:batch_end:2] = x_corrupted\n",
    "    else:\n",
    "        if x_corrupted.size()[1] != L: raise ValueError(f'warning: in hook {hook.name} with input_hook_name {input_hook_name} you are patching on position in the second index but size is {x_corrupted.size()}')\n",
    "        x[batch_start:batch_end:2,position] = x_corrupted[:,position]\n",
    "    return x\n",
    "\n",
    "\n",
    "def overwrite_h_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    position: int = None,\n",
    "):\n",
    "    x[batch_start:batch_end:2] = x[batch_start+1:batch_end:2]\n",
    "    return x\n",
    "\n",
    "# we do a hacky thing where this first hook clears the global storage\n",
    "# second hook stores all the hooks\n",
    "# then third hook computes the output (over all the hooks)\n",
    "# this avoids recomputing and so is much faster\n",
    "CONV_HOOKS = \"conv hooks\"\n",
    "CONV_BATCHES = \"conv batches\"\n",
    "def conv_patching_init_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    **kwargs\n",
    "):\n",
    "    # we need to clear this here\n",
    "    # i tried having a \"current layer\" variable in the conv_storage that only clears when it doesn't match\n",
    "    # but that doesn't work if you only patch the same layer over and over,\n",
    "    # as stuff gets carried over\n",
    "    # this way of doing things is much safer and lets us assume it'll be empty\n",
    "    # well not quite, note that conv_patching_hook will be called with different batch_start and batch_end inputs during one forward pass\n",
    "    # so we need to account for that in the keys we use\n",
    "    global conv_storage\n",
    "    conv_storage = {CONV_BATCHES: set()}\n",
    "    return x\n",
    "\n",
    "# hook h has a weird index!!!!!\n",
    "\n",
    "def conv_patching_storage_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    conv_filter_i: int,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    global storage\n",
    "    storage[hook.name] = x\n",
    "    global conv_storage\n",
    "    hooks_key = (CONV_HOOKS, batch_start, batch_end)\n",
    "    if not hooks_key in conv_storage:\n",
    "        conv_storage[hooks_key] = [] # we can't do this above because it'll be emptied again on the next batch before this is called\n",
    "    conv_storage[hooks_key].append({\"position\": position, \"conv_filter_i\": conv_filter_i})\n",
    "    conv_storage[CONV_BATCHES].add((batch_start, batch_end))\n",
    "    return x\n",
    "\n",
    "from jaxtyping import Float\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "global storage_for_grad\n",
    "\n",
    "global conv_storage\n",
    "def conv_patching_hook(\n",
    "    conv_output: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    layer: int,\n",
    "    computing_attribution: bool,\n",
    "    **kwargs,\n",
    ") -> Float[torch.Tensor, \"B L E\"]:\n",
    "    global conv_storage\n",
    "    global storage\n",
    "    ### This is identical to what the conv is doing\n",
    "    # but we break it apart so we can patch on individual filters\n",
    "\n",
    "    # we have two input hooks, the second one is the one we want\n",
    "    input_hook_name = input_hook_name[1]\n",
    "    \n",
    "    D_CONV = model.cfg.d_conv\n",
    "\n",
    "    # given batches like [(2,4), (5,6)] and total size 7 this returns (0,2), (4,5), (6,7) \n",
    "    def get_missing_batches(batches, B):\n",
    "        covered_i = torch.zeros([B])\n",
    "        for batch_start, batch_end in batches:\n",
    "            covered_i[batch_start:batch_end] = 1\n",
    "    \n",
    "        missing_batches = []\n",
    "        missing_start = 0\n",
    "        for i in range(B):\n",
    "            if covered_i[i] == 1:\n",
    "                if i != missing_start:\n",
    "                    missing_batches.append((missing_start, i))\n",
    "                missing_start = i+1\n",
    "        if covered_i[B-1] == 0:\n",
    "            missing_batches.append((missing_start, B))\n",
    "        return missing_batches\n",
    "    \n",
    "\n",
    "    \n",
    "    # [E,1,D_CONV]\n",
    "    conv_weight = model.blocks[layer].conv1d.weight\n",
    "    # [E]\n",
    "    conv_bias = model.blocks[layer].conv1d.bias\n",
    "    \n",
    "    # don't recompute these if we don't need to\n",
    "    # because we stored all the hooks and batches in conv_storage, we can just do them all at once\n",
    "    output_key = f'output' # they need to share an output because they write to the same output tensor\n",
    "    if not output_key in conv_storage:\n",
    "        #print(\"layer\", layer, \"keys\", conv_storage)\n",
    "        apply_to_all_hooks = [] # this is important because otherwise the [0:None] would overwrite the previous results (or vice versa)\n",
    "        apply_to_all_key = (CONV_HOOKS, 0, None)\n",
    "        if apply_to_all_key in conv_storage:\n",
    "            apply_to_all_hooks = conv_storage[apply_to_all_key]\n",
    "            # we need to do this so it applies to the other batches that we aren't otherwise patching\n",
    "            for batch_start, batch_end in get_missing_batches(conv_storage[CONV_BATCHES], conv_input.size()[0]):\n",
    "                conv_storage[CONV_BATCHES].add(batch_start, batch_end)\n",
    "                conv_storage[(CONV_HOOKS, batch_start, batch_end)] = []\n",
    "        for batch_start, batch_end in conv_storage[CONV_BATCHES]:\n",
    "            if batch_start == 0 and batch_end == None: continue # we cover this in the apply to all hooks above\n",
    "            def get_filter_key(i):\n",
    "                return f'filter_{i}'\n",
    "            conv_input_uncorrupted = storage[input_hook_name][batch_start:batch_end:2]\n",
    "            conv_input_corrupted = storage[input_hook_name][batch_start+1:batch_end:2]\n",
    "            B, L, E = conv_input_uncorrupted.size()\n",
    "            \n",
    "            conv_input_uncorrupted = rearrange(conv_input_uncorrupted, 'B L E -> B E L')\n",
    "            conv_input_corrupted = rearrange(conv_input_corrupted, 'B L E -> B E L')\n",
    "            \n",
    "            # pad zeros in front\n",
    "            # [B,E,D_CONV-1+L]\n",
    "            padded_input_uncorrupted = torch.nn.functional.pad(conv_input_uncorrupted, (D_CONV-1,0), mode='constant', value=0)\n",
    "            padded_input_corrupted = torch.nn.functional.pad(conv_input_corrupted, (D_CONV-1,0), mode='constant', value=0)\n",
    "    \n",
    "            # compute the initial filter values\n",
    "            for i in range(D_CONV):\n",
    "                filter_key = get_filter_key(i)\n",
    "                # [B,E,L]                      [E,1]                      [B,E,L]\n",
    "                filter_contribution = conv_weight[:,0,i].view(E,1)*padded_input_uncorrupted[:,:,i:i+L]\n",
    "                conv_storage[filter_key] = filter_contribution\n",
    "            \n",
    "            # apply all the hooks\n",
    "            for hook in conv_storage[(CONV_HOOKS, batch_start, batch_end)] + apply_to_all_hooks:\n",
    "                position = hook['position']\n",
    "                conv_filter_i = hook['conv_filter_i']\n",
    "                #print(f\"position {position} conv_filter_i {conv_filter_i} batch_start {batch_start} batch_end {batch_end}\")\n",
    "                filter_key = get_filter_key(conv_filter_i)\n",
    "                # [1,E,L]                                   [E,1]                          # [B,E,L]\n",
    "                corrupted_filter_contribution = conv_weight[:,0,conv_filter_i].view(E,1)*padded_input_corrupted[:,:,conv_filter_i:conv_filter_i+L]\n",
    "                filter_contribution = conv_storage[filter_key]\n",
    "                if position is None:\n",
    "                    # [B,E,L]                    [B,E,L]\n",
    "                    filter_contribution = corrupted_filter_contribution\n",
    "                else:\n",
    "                    # [B,E]                                                  [B,E]\n",
    "                    filter_contribution[:,:,position] = corrupted_filter_contribution[:,:,position]\n",
    "                conv_storage[filter_key] = filter_contribution\n",
    "            \n",
    "            # compute the output\n",
    "            output = torch.sum([conv_storage[get_filter_key(i)] for i in range(D_CONV)])\n",
    "            #output = torch.zeros([B,E,L], device=model.cfg.device)\n",
    "            #print(f'B {B} B2 {B2} E {E} L {L} conv_storage keys {conv_storage.keys()} filter sizes {[(k,v.size()) for (k,v) in conv_storage.items() if not type(v) is int]}')\n",
    "            for i in range(D_CONV):\n",
    "                filter_key = get_filter_key(i)\n",
    "                #output += conv_storage[filter_key]\n",
    "                del conv_storage[filter_key] # clean up now we are done with it, just to be safe\n",
    "                \n",
    "            # bias is not dependent on input so no reason to patch on it, just apply it as normal\n",
    "            output += conv_bias.view(E, 1)\n",
    "            output = rearrange(output, 'B E L -> B L E')\n",
    "            # interleave it back with the corrupted as every other\n",
    "            conv_output[batch_start:batch_end:2] = output\n",
    "        conv_storage[output_key] = conv_output\n",
    "    return conv_storage[output_key]\n",
    "\n",
    "\n",
    "limited_layers = list(range(model.cfg.n_layers))\n",
    "\n",
    "\n",
    "from acdc.data.ioi import ioi_data_generator, ABC_TEMPLATES, get_all_single_name_abc_patching_formats\n",
    "from acdc.data.utils import generate_dataset\n",
    "\n",
    "num_patching_pairs = 200\n",
    "seed = 27\n",
    "valid_seed = 28\n",
    "constrain_to_answers = True\n",
    "# this makes our data size 800, first 400 is each (a,b) pair, and then second 400 is each pair swapped to be (b,a)\n",
    "has_symmetric_patching = True\n",
    "\n",
    "from acdc.data.ioi import BABA_TEMPLATES\n",
    "templates = ABC_TEMPLATES\n",
    "patching_formats = list(get_all_single_name_abc_patching_formats())\n",
    "\n",
    "# generate data once to populate good_names and good_nouns (holds single-token choices)\n",
    "data = generate_dataset(model=model,\n",
    "                  data_generator=ioi_data_generator,\n",
    "                  num_patching_pairs=num_patching_pairs,\n",
    "                  seed=seed,\n",
    "                  valid_seed=valid_seed,\n",
    "                  constrain_to_answers=constrain_to_answers,\n",
    "                  has_symmetric_patching=has_symmetric_patching, \n",
    "                  varying_data_lengths=True,\n",
    "                  templates=templates,\n",
    "                  patching_formats=patching_formats)\n",
    "\n",
    "import acdc.data.ioi\n",
    "from collections import defaultdict\n",
    "name_positions_map = defaultdict(lambda: [])\n",
    "for template in templates:\n",
    "    name = acdc.data.ioi.good_names[0]\n",
    "    template_filled_in = template.replace(\"[NAME]\", name)\n",
    "    template_filled_in = template_filled_in.replace(\"[PLACE]\", acdc.data.ioi.good_nouns['[PLACE]'][0])\n",
    "    template_filled_in = template_filled_in.replace(\"[OBJECT]\", acdc.data.ioi.good_nouns['[OBJECT]'][0])\n",
    "    # get the token positions of the [NAME] in the prompt\n",
    "    name_positions = tuple([(i) for (i,s) in enumerate(model.to_str_tokens(torch.tensor(model.tokenizer.encode(template_filled_in)))) if s == f' {name}'])\n",
    "    name_positions_map[name_positions].append(template)\n",
    "sorted_by_frequency = sorted(list(name_positions_map.items()), key=lambda x: -len(x[1]))\n",
    "most_frequent_name_positions, templates = sorted_by_frequency[0]\n",
    "print(\"using templates\")\n",
    "for template in templates:\n",
    "    print(template)\n",
    "print(f\"with name positions {most_frequent_name_positions}\")\n",
    "\n",
    "data = generate_dataset(model=model,\n",
    "              data_generator=ioi_data_generator,\n",
    "              num_patching_pairs=num_patching_pairs,\n",
    "              seed=seed,\n",
    "              valid_seed=valid_seed,\n",
    "              constrain_to_answers=constrain_to_answers,\n",
    "              has_symmetric_patching=has_symmetric_patching, \n",
    "              varying_data_lengths=True,\n",
    "              templates=templates,\n",
    "              patching_formats=patching_formats)\n",
    "\n",
    "\n",
    "## Setup edges for ACDC\n",
    "edges = []\n",
    "B,L = data.data.size()\n",
    "#positions = list(range(L)) # \n",
    "positions = [None]\n",
    "\n",
    "INPUT_HOOK = f'hook_embed'\n",
    "INPUT_NODE = 'embed'\n",
    "\n",
    "last_layer = model.cfg.n_layers-1\n",
    "OUTPUT_HOOK = f'blocks.{last_layer}.hook_resid_post'\n",
    "OUTPUT_NODE = 'output'\n",
    "\n",
    "def input(layer):\n",
    "    return f'{layer}.i'\n",
    "\n",
    "def output(layer):\n",
    "    return f'{layer}.o'\n",
    "\n",
    "def conv(layer):\n",
    "    return f'{layer}.conv'\n",
    "\n",
    "def skip(layer):\n",
    "    return f'{layer}.skip'\n",
    "\n",
    "def ssm(layer):\n",
    "    return f'{layer}.ssm'\n",
    "\n",
    "# important to have storage be global and not passed into the hooks! Otherwise it gets very slow (tbh, i don't know why)\n",
    "global storage\n",
    "storage = {}\n",
    "\n",
    "for pos in positions:\n",
    "    # direct connections from embed to output\n",
    "    edges.append(Edge(\n",
    "            label=str(pos),\n",
    "            input_node=INPUT_NODE,\n",
    "            input_hook=(INPUT_HOOK, storage_hook),\n",
    "            output_node=OUTPUT_NODE,\n",
    "            output_hook=(OUTPUT_HOOK, partial(resid_patching_hook, position=pos)),\n",
    "    ))\n",
    "\n",
    "for layer in limited_layers:\n",
    "    for pos_i, pos in enumerate(positions):\n",
    "        # edge from embed to layer input\n",
    "        edges.append(Edge(\n",
    "                label=str(pos),\n",
    "                input_node=INPUT_NODE,\n",
    "                input_hook=(INPUT_HOOK, partial(storage_hook)),\n",
    "                output_node=input(layer),\n",
    "                output_hook=(f'blocks.{layer}.hook_layer_input', partial(resid_patching_hook, position=pos)),\n",
    "        ))\n",
    "\n",
    "        # input to conv\n",
    "        for conv_i in range(model.cfg.d_conv):\n",
    "            edges.append(Edge(\n",
    "                    label=(f'[{pos}:{conv_i-model.cfg.d_conv+1}]'.replace(\"None:\", \"\")), # [-D_CONV+1, -D_CONV+2, ..., -2, -1, 0]\n",
    "                    input_node=input(layer),\n",
    "                    input_hook=[\n",
    "                        (f'blocks.{layer}.hook_layer_input', conv_patching_init_hook),\n",
    "                        (f'blocks.{layer}.hook_in_proj', partial(conv_patching_storage_hook, position=pos, layer=layer, conv_filter_i=conv_i))\n",
    "                    ],\n",
    "                    output_node=conv(layer),\n",
    "                    output_hook=(f'blocks.{layer}.hook_conv', partial(conv_patching_hook, position=pos, layer=layer, conv_filter_i=conv_i)),\n",
    "            ))\n",
    "        \n",
    "        # conv to ssm\n",
    "        if pos is None:\n",
    "            # we need a seperate hook for each pos, but put them all into one edge\n",
    "            hooks = []\n",
    "            for other_pos in range(L):\n",
    "                hooks.append((f'blocks.{layer}.hook_h.{other_pos}', overwrite_h_hook))\n",
    "            edges.append(Edge(\n",
    "                    input_node=conv(layer),\n",
    "                    output_node=ssm(layer),\n",
    "                    output_hook=hooks,\n",
    "            ))\n",
    "        else:\n",
    "            edges.append(Edge(\n",
    "                    label=f'{pos}',\n",
    "                    input_node=conv(layer),\n",
    "                    output_node=ssm(layer),\n",
    "                    output_hook=(f'blocks.{layer}.hook_h.{pos}', overwrite_h_hook),\n",
    "            ))\n",
    "\n",
    "        if pos_i == 0: # we only need one of these\n",
    "            # ssm to output\n",
    "            edges.append(Edge(\n",
    "                    input_node=ssm(layer),\n",
    "                    output_node=output(layer),\n",
    "            ))\n",
    "        \n",
    "        # input to skip\n",
    "        edges.append(Edge(\n",
    "                label=f'{pos}',\n",
    "                input_node=input(layer),\n",
    "                output_node=skip(layer),\n",
    "                output_hook=(f'blocks.{layer}.hook_skip', partial(overwrite_patching_hook, position=pos)),\n",
    "        ))\n",
    "\n",
    "        if pos_i == 0: # we only need one of these\n",
    "            # skip to output\n",
    "            edges.append(Edge(\n",
    "                    input_node=skip(layer),\n",
    "                    output_node=output(layer),\n",
    "            ))\n",
    "        \n",
    "        for later_layer in limited_layers:\n",
    "                if layer < later_layer:\n",
    "                    # edge from layer output to other layer input\n",
    "                    edges.append(Edge(\n",
    "                            label=str(pos),\n",
    "                            input_node=output(layer),\n",
    "                            input_hook=(f'blocks.{layer}.hook_out_proj', storage_hook),\n",
    "                            output_node=input(later_layer),\n",
    "                            output_hook=(f'blocks.{later_layer}.hook_layer_input', partial(resid_patching_hook, position=pos)),\n",
    "                    ))\n",
    "        \n",
    "        # edge from layer output to final layer output\n",
    "        edges.append(Edge(\n",
    "                label=str(pos),\n",
    "                input_node=output(layer),\n",
    "                input_hook=(f'blocks.{layer}.hook_out_proj', storage_hook),\n",
    "                output_node=OUTPUT_NODE,\n",
    "                output_hook=(OUTPUT_HOOK, partial(resid_patching_hook, position=pos)),\n",
    "        ))\n",
    "\n",
    "# we have edges between layers\n",
    "# now, that is edges between output(input_layer) and input(output_layer)\n",
    "# we can then allow pruning to remove stranded nodes\n",
    "\n",
    "keeping_edges = set()\n",
    "for score, (input_layer, output_layer, pos) in pruned_edges:\n",
    "    keeping_edges.add((str(input_layer), str(output_layer)))\n",
    "\n",
    "print(sorted(list(keeping_edges)))\n",
    "\n",
    "totes_keep = []\n",
    "for edge in edges:\n",
    "    is_between_layers = False\n",
    "    layer_input = None\n",
    "    layer_output = None\n",
    "    if edge.input_node == INPUT_NODE:\n",
    "        is_between_layers = True\n",
    "        layer_input = 'embed'\n",
    "    if edge.output_node == OUTPUT_NODE:\n",
    "        is_between_layers = True\n",
    "        layer_output = 'output'\n",
    "\n",
    "    if '.' in edge.input_node:\n",
    "        input_layer, input_type = edge.input_node.split(\".\")\n",
    "        if edge.input_node == output(input_layer):\n",
    "            is_between_layers = True\n",
    "            layer_input = str(input_layer)\n",
    "    if '.' in edge.output_node:\n",
    "        output_layer, output_type = edge.output_node.split(\".\")\n",
    "        if edge.output_node == input(output_layer):\n",
    "            is_between_layers = True\n",
    "            layer_output = str(output_layer)\n",
    "    if is_between_layers:\n",
    "        if (layer_input, layer_output) in keeping_edges:\n",
    "            edge.patching = False\n",
    "            edge.checked = False\n",
    "            totes_keep.append((edge.input_node, edge.output_node))\n",
    "        else:\n",
    "            edge.patching = True\n",
    "            edge.checked = True\n",
    "    else:\n",
    "        edge.patching = False\n",
    "        edge.checked = False\n",
    "\n",
    "print(list(sorted(totes_keep)))\n",
    "\n",
    "model_kwargs = {\n",
    "    'fast_ssm': False,\n",
    "    'fast_conv': False,\n",
    "}    \n",
    "\n",
    "def normalized_logit_diff_acdc_metric(data: ACDCEvalData, printing=False):\n",
    "    B,V = data.patched.logits.size()\n",
    "\n",
    "    # [batch_size]\n",
    "    patched_correct = data.patched.correct[:,0]\n",
    "    #print(data.unpatched.logits.size(), data.patched.logits.size(), data.corrupted.logits.size())\n",
    "    A_logits_unpatched = data.unpatched.logits[torch.arange(B), patched_correct]\n",
    "    A_logits_patched = data.patched.logits[torch.arange(B), patched_correct]\n",
    "    A_logits_corrupted = data.corrupted.logits[torch.arange(B), patched_correct]\n",
    "\n",
    "    corrupted_correct = data.corrupted.correct[:,0]\n",
    "    B_logits_unpatched = data.unpatched.logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_patched = data.patched.logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_corrupted = data.corrupted.logits[torch.arange(B), corrupted_correct]\n",
    "\n",
    "    # A and B are two potential outputs\n",
    "    # if A patched > B patched, we are correct\n",
    "    # else we are incorrect\n",
    "\n",
    "    # thus we could just return A_logits_patched - B_logits_patched\n",
    "\n",
    "    # however it is useful to \"normalize\" these values\n",
    "\n",
    "    # in the worst case, our patching causes us to act like corrupted, and our diff will be\n",
    "    # A_logits_corrupted - B_logits_corrupted\n",
    "    # this will result in a small, negative value\n",
    "    \n",
    "    # in the best case, our patching will do nothing (cause us to act like unpatched), and our diff will be\n",
    "    # A_logits_unpatched - B_logits_unpatched\n",
    "    # this will result in a large, positive value\n",
    "\n",
    "    # thus we can treat those as the \"min\" and \"max\" and normalize accordingly\n",
    "    \n",
    "    min_diff = A_logits_corrupted - B_logits_corrupted\n",
    "    max_diff = A_logits_unpatched - B_logits_unpatched\n",
    "\n",
    "    possible_range = (max_diff-min_diff)\n",
    "    possible_range[possible_range == 0] = 1.0 # prevent divide by zero\n",
    "    \n",
    "    diff = A_logits_patched - B_logits_patched\n",
    "    normalized_diff = (diff-min_diff)/torch.abs(possible_range) # abs prevents incorrect data from wanting to be more incorrect\n",
    "\n",
    "    if printing:\n",
    "        print(f\"A corrupted {A_logits_corrupted}\")\n",
    "        print(f\"B corrupted {B_logits_corrupted}\")\n",
    "        print(f\"A unpatched {A_logits_unpatched}\")\n",
    "        print(f\"B unpatched {B_logits_unpatched}\")\n",
    "        print(f\"A patched {A_logits_patched}\")\n",
    "        print(f\"B patched {B_logits_patched}\")\n",
    "        print(f\"min diff {min_diff}\")\n",
    "        print(f\"max diff {max_diff}\")\n",
    "        print(f\"possible range {possible_range}\")\n",
    "        print(f\"diff {diff}\")\n",
    "        print(f\"normalized diff {normalized_diff}\")\n",
    "    # as described, 1.0 corresponds to acting like unpatched,\n",
    "    # and 0.0 corresponds to acting like corrupted\n",
    "\n",
    "    return torch.mean(normalized_diff)\n",
    "\n",
    "\n",
    "job_name_text = 'mamba acdc full 4' + \".txt\"\n",
    "wandb.login()\n",
    "acdc.tqdm = tqdm\n",
    "\n",
    "if os.path.exists(job_name_text):\n",
    "    with open(job_name_text, \"r\") as f:\n",
    "        wandb_id = f.read().strip()\n",
    "    print(f\"resuming from {wandb_id}\")\n",
    "    resume = True\n",
    "else:\n",
    "    wandb_id = wandb.util.generate_id()\n",
    "    with open(job_name_text, \"w\") as f:\n",
    "        f.write(wandb_id)\n",
    "    print(f\"starting new run {wandb_id}\")\n",
    "    resume = False\n",
    "\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "import acdc\n",
    "\n",
    "cfg = ACDCConfig(\n",
    "    ckpt_directory = wandb_id,\n",
    "    thresh = 0.0001,\n",
    "    rollback_thresh = 0.0001,\n",
    "    metric=normalized_logit_diff_acdc_metric,\n",
    "    # extra inference args\n",
    "    model_kwargs=model_kwargs,\n",
    "    # these are needed for doing graph pruning\n",
    "    input_node=INPUT_NODE,\n",
    "    output_node=OUTPUT_NODE,\n",
    "    # batch size for evaluating data points\n",
    "    batch_size=1,\n",
    "    log_level=LOG_LEVEL_INFO,\n",
    "    # if False, will be equivalent to batch_size=1\n",
    "    batched = True,\n",
    "    # set these two to false to use traditional ACDC\n",
    "    # recursive will try patching multiple at a time (this is faster sometimes)\n",
    "    recursive = True,\n",
    "    # try_patching_multiple_at_same_time will evaluate many different patchings before commiting to any\n",
    "    # and includes a rollback scheme if after patching one, the others get worse\n",
    "    try_patching_multiple_at_same_time = True,\n",
    "    ## if true, you metric will also have the logits from a run with no patching available\n",
    "    # (useful for normalized logit diff)\n",
    "    store_unpatched_logits = True,\n",
    ")\n",
    "\n",
    "wandb_resume = None\n",
    "if resume:\n",
    "    try:\n",
    "        most_recent_checkpoint_path, is_done = get_most_recent_checkpoint(checkpoint_dir=cfg.ckpt_directory)\n",
    "        if is_done:\n",
    "            print(f\"run {cfg.ckpt_directory} is completed\")\n",
    "        else:\n",
    "            print(f\"resuming from checkpoint {most_recent_checkpoint_path}\")\n",
    "            old_cfg, edges = load_checkpoint(path=most_recent_checkpoint_path)\n",
    "            cfg.iter = old_cfg.iter\n",
    "            wandb_resume = 'must'\n",
    "    except FileNotFoundError:\n",
    "        print(f\"no checkpoints available at {cfg.ckpt_directory}, starting from scratch\")\n",
    "class InterruptedException(Exception):\n",
    "    pass\n",
    "\n",
    "def interrupt_callback(sig_num: Any, stack_frame: Any):\n",
    "    raise InterruptedException()\n",
    "\n",
    "try:\n",
    "    wandb.init(\n",
    "        project=\"mamba-acdc\",\n",
    "        config=cfg,\n",
    "        name=job_name_text.replace(\".txt\", \"\") + f' thresh={cfg.thresh} rollback_thresh={cfg.rollback_thresh}',\n",
    "        resume=wandb_resume,\n",
    "        id=wandb_id,\n",
    "    )\n",
    "    # signal handlers (if preempted)\n",
    "    signal.signal(signal.SIGINT, interrupt_callback)\n",
    "    signal.signal(signal.SIGTERM, interrupt_callback)\n",
    "    result_edges = run_acdc(model=model, data=data, cfg=cfg, edges=edges)\n",
    "except (KeyboardInterrupt, InterruptedException):\n",
    "    print(\"interrupted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d716bc32-b8d0-4320-8057-373638c8499b",
   "metadata": {},
   "source": [
    "# Display ACDC Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a72d39-8320-4c57-ba05-b3ebb97ac465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from acdc import ACDCEvalData\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "import torch\n",
    "from IPython.display import display, FileLink, Image\n",
    "import acdc\n",
    "from importlib import reload\n",
    "edges = result_edges\n",
    "by_filters = torch.zeros([model.cfg.d_conv+2, model.cfg.n_layers])\n",
    "\n",
    "# change this to get the different plots\n",
    "CAP = 0.01\n",
    "\n",
    "def filter_score(score):\n",
    "    # return score # uncomment this to have no cap\n",
    "    return min(score, CAP)\n",
    "for edge in edges:\n",
    "    if not edge.patching and edge.checked:\n",
    "        if '.conv' in edge.output_node:\n",
    "            filter = int(edge.label.split(\"]\")[0][1:])        \n",
    "            layer = edge.output_node.split(\".\")[0]\n",
    "            by_filters[abs(int(filter)),int(layer)] = filter_score(edge.score_diff_when_patched)\n",
    "        if '.skip' in edge.output_node:        \n",
    "            layer = edge.output_node.split(\".\")[0]\n",
    "            by_filters[model.cfg.d_conv,int(layer)] = filter_score(edge.score_diff_when_patched)\n",
    "        if '.ssm' in edge.output_node:\n",
    "            layer = edge.output_node.split(\".\")[0]\n",
    "            by_filters[model.cfg.d_conv+1,int(layer)] = filter_score(edge.score_diff_when_patched)\n",
    "            \n",
    "\n",
    "def layer_to_i(node):\n",
    "    if node == INPUT_NODE:\n",
    "        return 0\n",
    "    elif node == OUTPUT_NODE:\n",
    "        return model.cfg.n_layers+1 # because embed is 0\n",
    "    else:\n",
    "        return int(node)+1 # because embed is 0\n",
    "\n",
    "adj_mat = torch.zeros([model.cfg.n_layers+2, model.cfg.n_layers+2])\n",
    "for edge in edges:\n",
    "    if edge.patching: continue\n",
    "    is_between_layers = False\n",
    "    layer_input = None\n",
    "    layer_output = None\n",
    "    if edge.input_node == INPUT_NODE:\n",
    "        is_between_layers = True\n",
    "        layer_input = INPUT_NODE\n",
    "    if edge.output_node == OUTPUT_NODE:\n",
    "        is_between_layers = True\n",
    "        layer_output = OUTPUT_NODE\n",
    "\n",
    "    if '.' in edge.input_node:\n",
    "        input_layer, input_type = edge.input_node.split(\".\")\n",
    "        if edge.input_node == output(input_layer):\n",
    "            is_between_layers = True\n",
    "            layer_input = str(input_layer)\n",
    "    if '.' in edge.output_node:\n",
    "        output_layer, output_type = edge.output_node.split(\".\")\n",
    "        if edge.output_node == input(output_layer):\n",
    "            is_between_layers = True\n",
    "            layer_output = str(output_layer)\n",
    "    if is_between_layers:\n",
    "        adj_mat[layer_to_i(layer_input), layer_to_i(layer_output)] = filter_score(edge.score_diff_when_patched)\n",
    "        \n",
    "def to_str(lis):\n",
    "    return [str(x) for x in lis]\n",
    "x_labels = to_str([-x for x in range(model.cfg.d_conv)]) + ['skip'] + ['ssm']\n",
    "imshow(by_filters.T, y=to_str(range(model.cfg.n_layers)), x=x_labels, title='which parts of layer using (conv filters, skip, and/or conv), capped at 0.05', font_size=8)\n",
    "\n",
    "labels = ['embed'] + [str(x) for x in range(model.cfg.n_layers)] + ['output']\n",
    "imshow(adj_mat, y=labels, x=labels, title='adjacency matrix clamped to 0.01', font_size=8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
